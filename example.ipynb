{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d4b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairness as fair\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2465b46",
   "metadata": {},
   "source": [
    "# Example Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10928194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ejohn\\Documents\\Fairness\\fairness.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  accuracy = icu_data.groupby(demographic_group).apply(\n",
      "c:\\Users\\ejohn\\Documents\\Fairness\\fairness.py:40: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  f1_scores = icu_data.groupby(demographic_group).apply(\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'predicted_mortality': np.random.randint(0, 2, size=1000),\n",
    "    'actual_mortality': np.random.randint(0, 2, size=1000),\n",
    "    'ethnicity': np.random.choice(['White', 'Black', 'Hispanic', 'Asian'], size=1000),\n",
    "    'gender': np.random.choice(['Male', 'Female'], size=1000)\n",
    "})\n",
    "\n",
    "demographic = 'ethnicity'\n",
    "fairness_threshold = 0.05\n",
    "predicted_outcome = 'predicted_mortality'\n",
    "actual_outcome = 'actual_mortality'\n",
    "\n",
    "plt.ioff()  # Disable interactive mode for matplotlib\n",
    "\n",
    "evaluated_metrics = fair.evaluate_fairness(\n",
    "    df,\n",
    "    demographic,\n",
    "    predicted_outcome,\n",
    "    actual_outcome,\n",
    "    fairness_threshold\n",
    ")\n",
    "\n",
    "plt.close('all')  # Close all matplotlib figures to avoid display issues in some environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7565d",
   "metadata": {},
   "source": [
    "# Accuracy Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb5ad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by demographic group:\n",
      "ethnicity\n",
      "Asian       0.514523\n",
      "Black       0.488372\n",
      "Hispanic    0.470833\n",
      "White       0.532567\n",
      "dtype: float64\n",
      "Disparity: 0.061733716475095846\n",
      "Is the model fair? False\n"
     ]
    }
   ],
   "source": [
    "accuracy_result = evaluated_metrics['accuracy']\n",
    "print(\"Accuracy by demographic group:\")\n",
    "print(accuracy_result['accuracy'])\n",
    "print(\"Disparity:\", accuracy_result['disparity'])    \n",
    "print(\"Is the model fair?\", accuracy_result['is_fair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f0eff",
   "metadata": {},
   "source": [
    "# F1 Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a67bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score by demographic group:\n",
      "ethnicity\n",
      "Asian       0.518519\n",
      "Black       0.480315\n",
      "Hispanic    0.477366\n",
      "White       0.500000\n",
      "dtype: float64\n",
      "Disparity: 0.041152263374485576\n",
      "Is the model fair? True\n"
     ]
    }
   ],
   "source": [
    "f1_result = evaluated_metrics['f1_score']\n",
    "print(\"\\nF1 Score by demographic group:\")\n",
    "print(f1_result['f1_scores'])    \n",
    "print(\"Disparity:\", f1_result['disparity'])    \n",
    "print(\"Is the model fair?\", f1_result['is_fair'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21e0c0",
   "metadata": {},
   "source": [
    "# Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7faabf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TPR by demographic group:\n",
      "ethnicity\n",
      "Asian       0.543103\n",
      "Black       0.504132\n",
      "Hispanic    0.495726\n",
      "White       0.508333\n",
      "dtype: float64\n",
      "Disparity: 0.04737695254936636\n",
      "\n",
      "FPR by demographic group:\n",
      "ethnicity\n",
      "Asian       0.512000\n",
      "Black       0.525547\n",
      "Hispanic    0.552846\n",
      "White       0.446809\n",
      "dtype: float64\n",
      "Disparity: 0.10603701781698671\n",
      "Is the model fair? False\n"
     ]
    }
   ],
   "source": [
    "equalized_odds_result = evaluated_metrics['equalized_odds']\n",
    "print(\"\\nTPR by demographic group:\")\n",
    "print(equalized_odds_result['tpr'])    \n",
    "print(\"Disparity:\", equalized_odds_result['tpr_disparity'])    \n",
    "print(\"\\nFPR by demographic group:\")\n",
    "print(equalized_odds_result['fpr'])    \n",
    "print(\"Disparity:\", equalized_odds_result['fpr_disparity'])  \n",
    "print(\"Is the model fair?\", equalized_odds_result['is_fair'])   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09e1f5",
   "metadata": {},
   "source": [
    "# Disparate Impact Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "357e8e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Disparate Impact Ratio by demographic group:\n",
      "ethnicity\n",
      "Asian       0.526971\n",
      "Black       0.515504\n",
      "Hispanic    0.525000\n",
      "White       0.475096\n",
      "Name: predicted_mortality, dtype: float64\n",
      "Ratio: 1.109188863605943\n",
      "Is the model fair? False\n"
     ]
    }
   ],
   "source": [
    "disparate_impact_result = evaluated_metrics['disparate_impact_ratio']\n",
    "print(\"\\nDisparate Impact Ratio by demographic group:\")\n",
    "print(disparate_impact_result['positive_prediction_rate'])    \n",
    "print(\"Ratio:\", disparate_impact_result['disparity'])    \n",
    "print(\"Is the model fair?\", disparate_impact_result['is_fair'])   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
